{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# BERT-Word2Vec Hybrid Keyphrase Extractor\n",
        "\n",
        "BERT has seen some extensive use in the kephrase extraction domain, and proves to be one of the best present keyphrase extraction tools, as seen with the [KeyBERT](https://github.com/MaartenGr/KeyBERT) model. This however is not enough, as it doesn't cover Absent Keyphrases which are phrases that do not exist in the text but hold semantic value nonetheless. This is where Word2Vec can be used to generate these keyphrases. The following model was tested using the Inspec dataset."
      ],
      "metadata": {
        "id": "-Y7Met-cdasu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install keybert"
      ],
      "metadata": {
        "id": "mSbElT0EeTXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gcNY4jp6eLwR"
      },
      "outputs": [],
      "source": [
        "from keybert import KeyBERT\n",
        "from pathlib import Path\n",
        "import glob\n",
        "import os\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models import KeyedVectors\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sentence_transformers import SentenceTransformer, util"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Wj3q6DyUPhTN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Global variables\n",
        "top_n = 5\n",
        "word2vec_model_path = '/content/drive/MyDrive/word2vec/GoogleNews-vectors-negative300.bin' #provide path to GoogleNews-vectors-negative300.bin"
      ],
      "metadata": {
        "id": "1KcIgY1gY_Ju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Present Keyphrase Extraction\n",
        "The following cell utilizes the pretrained KeyBERT model to extract present keyphrases in a document."
      ],
      "metadata": {
        "id": "aoNpRonKu5oT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PKey_Extraction:\n",
        "\n",
        "    def __init__(self, input_dataset='../Inspec/docsutf8/'):\n",
        "        self.input_dataset= input_dataset\n",
        "\n",
        "        #check if output directories exist...\n",
        "        Path(\"./Output/\").mkdir(parents=True, exist_ok=True)\n",
        "        Path(\"./Output/AKE\").mkdir(parents=True, exist_ok=True)\n",
        "        Path(\"./Output/PKE/\").mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        self.kw_model = KeyBERT()\n",
        "\n",
        "    def extract_presentKeyphrases(self):\n",
        "      counter = 0\n",
        "      #iterate over all files in the dataset ...\n",
        "      fNames= glob.glob(self.input_dataset + '/*txt')\n",
        "      for file in fNames:\n",
        "        counter += 1\n",
        "        if counter > 500:\n",
        "          break\n",
        "        print(f\"Processing file: {file}\")\n",
        "        # read the content of the input document.\n",
        "        input_doc = open(file, mode='r').read()\n",
        "        input_doc=input_doc.replace('\\t', ' ').replace('\\n', ' ')\n",
        "\n",
        "        # extract present keyphrases\n",
        "        keywords = self.kw_model.extract_keywords(input_doc, keyphrase_ngram_range=(1, 1),\n",
        "                                        stop_words='english', use_mmr=True, diversity=0.5, top_n=top_n)\n",
        "\n",
        "        # save keywods without relevance score into file\n",
        "        final_keywords=\"\"\n",
        "        for keyword in keywords:\n",
        "          final_keywords+=keyword[0]+\"\\n\"\n",
        "\n",
        "        with open('./Output/PKE/'+file.split('/')[-1], 'w') as outFile:\n",
        "          outFile.writelines(final_keywords.rstrip())\n",
        "        print(f\"Keyphrases written to {outFile.name}\")\n",
        "        outFile.close()"
      ],
      "metadata": {
        "id": "Olz96Qudemh8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Extractor = PKey_Extraction(input_dataset='/content/drive/MyDrive/Inspec/docsutf8') #provide path to docutf8 folder in Inspec"
      ],
      "metadata": {
        "id": "C_-BaKUgg_mn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keywords = Extractor.extract_presentKeyphrases()"
      ],
      "metadata": {
        "id": "mQKgIfiHh5Q5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Absent Keyphrase Generation\n",
        "The 3 functions and code cells below are used for Absent Keyphrase generation. The most_similar() function in Word2vec is used to generate absent keyphrases from the present keyphrases that are fed in as parameters."
      ],
      "metadata": {
        "id": "MpHTDUxqQAr5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-trained Word2Vec model from google drive\n",
        "word2vec_model = KeyedVectors.load_word2vec_format(word2vec_model_path, binary=True)"
      ],
      "metadata": {
        "id": "GtjI4DgYm0YX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Find similar words using Word2Vec\n",
        "def word2vec(keywords):\n",
        "  similar_words = {}\n",
        "  for word in keywords:\n",
        "    if word not in word2vec_model.key_to_index:\n",
        "      continue\n",
        "    similar_words.update(word2vec_model.most_similar(word, topn=top_n))\n",
        "  return similar_words"
      ],
      "metadata": {
        "id": "hTECE03yfdvq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def postprocess(words, doc):\n",
        "  words = list(words.keys())\n",
        "  lowercase_words = [word.lower().replace('_', ' ') for word in words]\n",
        "  absents = []\n",
        "  for word in lowercase_words:\n",
        "    if word not in doc:\n",
        "      absents.append(word)\n",
        "  return absents"
      ],
      "metadata": {
        "id": "co1MwO4ING6E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def absentRelevance(absent_words, doc):\n",
        "  kw_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "  doc_embedding = kw_model.encode(doc)\n",
        "  word_embedding = kw_model.encode(absent_words)\n",
        "  cosine_score = util.pytorch_cos_sim(doc_embedding, word_embedding)\n",
        "  cosine_scores_list = cosine_score.flatten().tolist()\n",
        "  word_score_dict = dict(zip(absent_words, cosine_scores_list))\n",
        "  word_score_dict = dict(sorted(word_score_dict.items(), key=lambda item: item[1], reverse=True))\n",
        "  return word_score_dict"
      ],
      "metadata": {
        "id": "DJQUA62yVgIR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fNames1 = glob.glob('/content/Output/PKE' + '/*txt')\n",
        "for file_path in fNames1:\n",
        "  filename = os.path.basename(file_path)\n",
        "  present_phrases = []\n",
        "  with open(file_path, 'r') as file:\n",
        "    # Extend the present_phrases list with lines from this file\n",
        "    present_phrases.extend([line.strip() for line in file])\n",
        "  with open('/content/drive/MyDrive/Inspec/docsutf8/'+filename, 'r', encoding='utf-8') as file: #provide path to docutf8 folder in Inspec\n",
        "    # Read the entire content of the file\n",
        "    content = file.read()\n",
        "  word2vec_phrases = word2vec(present_phrases)\n",
        "  absents = postprocess(word2vec_phrases, content)\n",
        "  absent_phrases = absentRelevance(absents, content)\n",
        "  absent_phrases = list(absent_phrases.keys())[:top_n]\n",
        "\n",
        "  # save Absent keywords into file\n",
        "  absent_keywords=\"\"\n",
        "  for keyword in absent_phrases:\n",
        "    absent_keywords+=keyword+\"\\n\"\n",
        "\n",
        "  with open('./Output/AKE/'+filename.split('/')[-1], 'w') as outFile:\n",
        "    outFile.writelines(absent_keywords.rstrip())\n",
        "  print(f\"Keyphrases written to {outFile.name}\")\n",
        "  outFile.close()"
      ],
      "metadata": {
        "id": "Fyp6Z1pnHIfi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation\n",
        "The following cells are only for evaluation purposes, the absent and present keyphrases should be saved in the Output/AKE and Output/PKE directories respectively."
      ],
      "metadata": {
        "id": "kc73ZrXtFT30"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cosine_similarity(phrases, gold):\n",
        "  kw_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "  phrases_embedding = kw_model.encode(phrases)\n",
        "  gold_embedding = kw_model.encode(gold)\n",
        "  cosine_score = util.pytorch_cos_sim(phrases_embedding, gold_embedding)\n",
        "  cosine_scores_list = cosine_score.flatten().tolist()\n",
        "  return cosine_scores_list\n",
        "\n",
        "def compute_evaluation(cosine_scores):\n",
        "\n",
        "    num_of_Similar=0 #number of similar keyphrases, we add 1 to avoid division by zero (i.e., smoothing)\n",
        "\n",
        "    for similarty_score in cosine_scores:\n",
        "        if any(similarity_threshould > 0.8 for similarity_threshould in similarty_score):\n",
        "            num_of_Similar+=1\n",
        "\n",
        "    recall= num_of_Similar/len(cosine_scores)\n",
        "    precision= num_of_Similar/len(cosine_scores)\n",
        "\n",
        "\n",
        "    return round(precision, 3), round(recall, 3)"
      ],
      "metadata": {
        "id": "4tYwdcXeaeb9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fNames1 = glob.glob('/content/Output/PKE' + '/*txt')\n",
        "cos_sim = []\n",
        "for file_path in fNames1:\n",
        "  filename = os.path.basename(file_path)\n",
        "  filename = os.path.splitext(filename)[0]\n",
        "  with open(file_path, 'r') as file:\n",
        "    # Extend the present_phrases list with lines from this file\n",
        "    present = file.read()\n",
        "  with open('/content/Output/AKE/'+filename+'.txt', 'r', encoding='utf-8') as file:\n",
        "    # Read the entire content of the file\n",
        "    absent = file.read()\n",
        "  with open('/content/drive/MyDrive/Inspec/keys/'+filename+'.key', 'r', encoding='utf-8') as file: #provide path to keys folder in Inspec\n",
        "    # Read the entire content of the file\n",
        "    gold_keys = file.read()\n",
        "\n",
        "  cos_sim.append(cosine_similarity(present, gold_keys))\n",
        "precision, recall = compute_evaluation(cos_sim)"
      ],
      "metadata": {
        "id": "LFPeF_LU0e4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fNames1 = glob.glob('/content/Output/PKE' + '/*txt')\n",
        "cos_sim1 = []\n",
        "for file_path in fNames1:\n",
        "  filename = os.path.basename(file_path)\n",
        "  filename = os.path.splitext(filename)[0]\n",
        "  with open(file_path, 'r') as file:\n",
        "    # Extend the present_phrases list with lines from this file\n",
        "    present = file.read()\n",
        "  with open('/content/Output/AKE/'+filename+'.txt', 'r', encoding='utf-8') as file:\n",
        "    # Read the entire content of the file\n",
        "    absent = file.read()\n",
        "  with open('/content/drive/MyDrive/Inspec/keys/'+filename+'.key', 'r', encoding='utf-8') as file: #provide path to keys folder in Inspec\n",
        "    # Read the entire content of the file\n",
        "    gold_keys = file.read()\n",
        "\n",
        "  cos_sim1.append(cosine_similarity(absent, gold_keys))\n",
        "precision1, recall1 = compute_evaluation(cos_sim1)"
      ],
      "metadata": {
        "id": "Df3Xn7ioRrMu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Present keyphrases Recall score\n",
        "print(recall)"
      ],
      "metadata": {
        "id": "Nom1b0uaKFIY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Absent keyphrases Recall score\n",
        "print(recall1)"
      ],
      "metadata": {
        "id": "h1fSyN-WTUQA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}